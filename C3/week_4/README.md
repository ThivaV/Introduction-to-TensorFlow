# Week 4

## Sequence models and literature

* References:
    * [Laurences Generated Poetry](https://github.com/https-deeplearning-ai/tensorflow-1-public/blob/main/C3/W4/misc/Laurences_generated_poetry.txt)
* [Generating text using a character-based RNN](https://www.tensorflow.org/tutorials/sequences/text_generation)
* What is the name of the method used to tokenize a list of sentences?
* If a sentence has 120 tokens in it, and a Conv1D with 128 filters with a Kernal size of 5 is passed over it, what’s the output shape?
* What is the purpose of the embedding dimension?
* IMDB Reviews are either positive or negative. What type of loss function should be used in this scenario?
* If you have a number of sequences of different lengths, how do you ensure that they are understood when fed into a neural network?
* When predicting words to generate poetry, the more words predicted the more likely it will end up gibberish. Why?
* What is a major drawback of word-based training for text generation instead of character-based generation?
* How does an LSTM help understand meaning when words that qualify each other aren’t necessarily beside each other in a sentence?